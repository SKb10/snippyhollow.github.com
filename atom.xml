<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Exchangeable random experiments]]></title>
  <link href="http://snippyhollow.github.com/atom.xml" rel="self"/>
  <link href="http://snippyhollow.github.com/"/>
  <updated>2014-08-09T15:23:34+02:00</updated>
  <id>http://snippyhollow.github.com/</id>
  <author>
    <name><![CDATA[syhw]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[So you wanna try Deep Learning?]]></title>
    <link href="http://snippyhollow.github.com/blog/2014/08/09/so-you-wanna-try-deep-learning/"/>
    <updated>2014-08-09T13:37:00+02:00</updated>
    <id>http://snippyhollow.github.com/blog/2014/08/09/so-you-wanna-try-deep-learning</id>
    <content type="html"><![CDATA[<p>I’m keeping this post quick and dirty, but at least it’s out there. The gist of this post is that <a href="8a0f820261926e2f41cc">I put out a one file gist that does all the basics</a>, so that you can play around with it yourself. First of all, I would say that deep learning is simply kernel machines whose kernel we learn. That’s gross but that’s not totally false. Second of all, there is nothing magical about deep learning, just that we can efficiently train (GPUs, clusters) large models (millions of weights, billions if you want to make a Wired headline) on large datasets (millions of images, thousands of hours of speech, more if you’re GOOG/FB/AAPL/MSFT/NSA). I think a good part of the success of deep learning comes from the fact that practitionners are not affraid to go around beautiful mathematical principles to have their model work on whatever dataset and whatever task. But I disgress…</p>

<h2 id="what-is-a-deep-neural-network">What is a deep neural network?</h2>

<p>A series of matrix multiplications and non-linearities. You take your input $x$ in your features space, multiply it by a matrix $W$ (add biases $b$), apply a non-linearity (Rectified Linear Unit is fashionable these days, that’s $max(0, output)$, but $sigmoid$ and $tanh$ are OK too) and keep on doing that with other layers until you reach a classifier. For instance, you have a 3 layers ReLUs-based neural network with a softmax classifier on top? That gives:</p>

<script type="math/tex; mode=display">y = softmax(max(0, W_2.(max(0, W_1.(max(0, W_0.x + b_0))+ b_1)) + b_2))</script>

<p>There are all sorts of different mammals, with very strong specificities, but I think I just described a rat (or is it an <a href="https://en.wikipedia.org/wiki/Euarchontoglires">euarchontoglires</a>?).</p>

<h2 id="links-and-papers">Links and Papers</h2>

<p>I’m just dumping here a collection of links that I think everybody with an interest in deep learning should at least skim:</p>

<ul>
  <li>First, you should of course start with <a href="http://deeplearning.net/tutorial/">the deeplearning.net tutorials</a>, even though it’s pretty old. Overall, these are very good foundations nevertheless.</li>
  <li>If you want to get an intuition for <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">how NNs fold space with non-linearities</a> and <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">an online demo to play around with this concept</a>.</li>
  <li>These online demos were nice, right? They’re done by a guy who also wrote a pretty interesting <a href="http://karpathy.github.io/2014/07/03/feature-learning-escapades/">personnal history that concurs with my point-of-view on feature learning</a>.</li>
  <li>I’m going to advise you against it in a bit, but if you want to do RBM pre-training, <a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">this paper is a must-read</a></li>
  <li>If you want to do anything that has to deal with images, start <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">here</a> and <a href="http://arxiv.org/pdf/1311.2901.pdf">there</a>.</li>
  <li>If you want to do anything that has to deal with speech (I assume you know about speech coding, <a href="http://i.imgur.com/fA0QIQr.png">otherwise I did a crash course</a>), start <a href="http://www.cs.utoronto.ca/~gdahl/papers/dbnPhoneRec.pdf">here</a> and <a href="http://www.csri.utoronto.ca/~hinton/absps/googlerectified.pdf">there</a>.</li>
  <li>If you want to do NLP with deep learning, there are lots of hot papers right now, but you could start with <a href="http://leon.bottou.org/publications/pdf/jmlr-2011.pdf">NLP (almost) from scratch</a>.</li>
  <li>In any case, you should learn <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">practical stuff about SGD (must-read)</a>, <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf">learn about momentum</a>, and you can geek out about extensions (I’m fond of <a href="http://arxiv.org/pdf/1212.5701.pdf">Adadelta</a>). You should learn about <a href="http://arxiv.org/pdf/1207.0580.pdf">Dropout</a>, and maybe geek out about the variants (fast dropout, dropconnect…).</li>
  <li>If you like videos, <a href="https://www.youtube.com/watch?v=6WeyTUnbwQQ">Optimization I</a> and <a href="http://www.youtube.com/embed/cXzGpiUcvRI?vq=hd1080&amp;autoplay=1">Leon (1)</a> <a href="http://www.youtube.com/embed/4-hTxJAwr8U?vq=hd1080&amp;autoplay=1">Bottou’s (2)</a> <a href="http://www.youtube.com/embed/adXwym8Lakg?vq=hd1080&amp;autoplay=1">MLSS class (3)</a> are good introductions.</li>
  <li>Finally, if you want more, you can have a look at my <a href="https://pinboard.in/search/u:syhw?query=deeplearning">non-extensive collection of links on deep learning</a>. </li>
</ul>

<h2 id="stuff-youll-learn">Stuff you’ll learn</h2>

<p>There I’m getting totally subjective, because I’m telling you stuff that I learned the hard way.</p>

<h4 id="generic">Generic</h4>

<ul>
  <li>Always answer “Do you want more data?” with “Yes, please.”</li>
  <li>If something feels wrong, check your gradients with finite differences.</li>
  <li>For all gradient descent related stuff, first <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">RTFM</a>.</li>
  <li>When do we stop the training? Almost everybody does it but nobody speaks about it: <a href="https://en.wikipedia.org/wiki/Early_stopping">early stopping on a validation set</a>.</li>
  <li>If you use $tanh$ or $sigmoid$ activation units, <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf">initialize them well</a>, respectively with uniform weights in <script type="math/tex">[-\sqrt{\frac{6}{\mathrm{fan}_{in} + \mathrm{fan}_{out}}}, \sqrt{\frac{6}{\mathrm{fan}_{in} + \mathrm{fan}_{out}}}]</script> or $4$ times that.</li>
</ul>

<h4 id="unsupervised-pre-training">Unsupervised Pre-Training</h4>

<ul>
  <li>“What is unsupervised pre-training?” Using un-annotated data to initialize the network’s </li>
  <li>What is unsupervised pre-training doing? <a href="http://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf">“unsupervised pre-training guides the learning towards basins of attraction of minima that are better in terms of the underlying data distribution; the evidence from these results supports a regularization explanation for the effect of pre-training.”</a></li>
  <li>This is not needed if you have enough data.</li>
</ul>

<h4 id="dropout">Dropout</h4>

<ul>
  <li>“How do we approach a problem with the deep learning mindset?” You design an under-constrained over-capacity over-fitting hog (by being deep and wide, just barely tractable efficiently on your hardware), and you keep it in check by using Dropout.</li>
  <li>“What is Dropout?” Dropping hidden units randomly (usually with a binomial probability of 0.5) during training so that the networks learns to be “robust” and doesn’t learn stupid co-activations of units (a way to tell the network to not just learn to compress the training set).</li>
  <li>“What is Dropout doing exactly?” <a href="http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf">“the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix”</a>, <a href="http://papers.nips.cc/paper/4878-understanding-dropout.pdf">“Dropout performs gradient descent on-line with respect to both the training examples and the ensemble of all possible subnetworks. (…) The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights to prevent overfitting. Dropout provides immediately the magnitude of the regularization term which is scaled by the inputs and by the variance of the dropout variables.”</a></li>
  <li>“Sorry, what?” You know about <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">L2 regularization</a> right? So you know about <a href="http://ej.iop.org/images/1741-2552/9/5/056002/Full/jne427232f9_online.jpg">this picture</a>, where regularization means inflating the L2 (or L1) ball until it intersects your feasible set. Now imagine an ellipsis that has its moments matching the ones of the inverse of the Fisher information matrix of the data. You now have a picture of “kinda” what Dropout is doing.</li>
</ul>

<h2 id="practice">Practice</h2>

<p>I’d advise to start by using either <a href="http://torch.ch/">Torch</a> (Lua) or <a href="http://deeplearning.net/software/theano/">Theano</a> (Python), both nice libraries that do automatic differentiation.</p>

<p>I put together a <a href="https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc">single file simple deep neural network working on small datasets (Python)</a>, more for pedagogical purposes than production ready, but it runs relatively fast on GPUs thanks to Theano. So if you want to run it, install Theano (I use the <a href="http://deeplearning.net/software/theano/install.html#bleeding-edge-install-instructions">bleeding edge</a>). If you want to play around with it, look for <code>TODO</code> in the code and change values there. <a href="https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc#file-dnn-py-L567-L573">There are several datasets</a> that you can use. Also, you should play around with the parameters of <a href="https://gist.github.com/SnippyHolloW/8a0f820261926e2f41cc#file-dnn-py-L575-L577">this function</a>, and maybe try against the SVMs from scikit-learn. Finally, if you use Dropout, you will see improvement only on large-enough networks (&gt; 1000 units / layer, &gt; 3-4 layers). Here is the result on running this file (<code>python dnn.py</code>) with a small ($784\times200\times200\times10$) ReLU-based L2-regularized network on MNIST:</p>

<p><img src="http://i.imgur.com/M3COTRE.png" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>I didn’t talk about convolutional neural networks, nor recurrent neural networks, nor other beasts. That should be the next step for the passionate reader. This was just a primer on raw facts for basic deep learning. Depending on what people want, I can either explain function by function the file that I provided here, talk about different loss functions (learning embeddings, e.g. as <a href="https://code.google.com/p/word2vec/">word2vec</a>), recurrent neural nets, etc.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A random thought about ReLUs]]></title>
    <link href="http://snippyhollow.github.com/blog/2014/07/18/a-random-thought-about-relus/"/>
    <updated>2014-07-18T16:59:00+02:00</updated>
    <id>http://snippyhollow.github.com/blog/2014/07/18/a-random-thought-about-relus</id>
    <content type="html"><![CDATA[<p>We know ReLU rock, they’re fast to compute, they’re fast to converge (train), they combine well with dropout… When I transitioned to ReLU for good, I found out that in phones recognition the “hard sigmoids” (piecewise approximation with 5 pieces) are doing almost as well as ReLUs (e.g. $\approx$24% phone error rate on TIMIT for a given architecture vs 23.5% IIRC), and much better than “smooth” sigmoids (26%). I’ve been wondering for a few months about how much of the good performance of ReLUs comes from the fact that they have a hard 0, that propagates to the upper layer and makes the higher level activations sparser and sparser. Is it well known? Is this random thought in the bad part of the random walk on the posterior? ;-)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spikey Spheres]]></title>
    <link href="http://snippyhollow.github.com/blog/2014/07/17/spikey-spheres/"/>
    <updated>2014-07-17T14:42:00+02:00</updated>
    <id>http://snippyhollow.github.com/blog/2014/07/17/spikey-spheres</id>
    <content type="html"><![CDATA[<p>Just a quick blog post to say that if you didn’t read <a href="http://www.penzba.co.uk/cgi-bin/PvsNP.py?SpikeySpheres">“Spikey Spheres”</a> before, you should. And <a href="http://nbviewer.ipython.org/urls/gist.github.com/SnippyHolloW/9025964/raw/b2d266e7e19d64e0343fd899dfbc3e8ddc889269/SpikeySpheres?create=1">there is my IPython notebook that goes with it</a>. There is also <a href="http://djalil.chafai.net/blog/2013/07/14/a-cube-a-starfish-a-thin-shell-and-the-central-limit-theorem/">this connection with the central limit theorem</a> (and the $l^{\infty}$ ball).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing phn2vec with biclustering]]></title>
    <link href="http://snippyhollow.github.com/blog/2014/06/20/visualizing-phn2vec-with-biclustering/"/>
    <updated>2014-06-20T12:17:00+02:00</updated>
    <id>http://snippyhollow.github.com/blog/2014/06/20/visualizing-phn2vec-with-biclustering</id>
    <content type="html"><![CDATA[<p>Following <a href="http://snippyhollow.github.io/blog/2014/05/27/phn2vec-embeddings/">my previous blog post on phn2vec</a>, I used <a href="http://scikit-learn.org/stable/modules/biclustering.html">scikit-learn’s biclustering</a> to make the similarity matrices more readable. So here are some quick results for TIMIT:</p>

<h2 id="phonetic-annotation">Phonetic annotation</h2>

<h3 id="biclusters">2 biclusters</h3>

<p>We clearly see consonants vs. vowels.</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_2_biclusters.png" /></p>

<h3 id="biclusters-1">4 biclusters</h3>

<p>We clearly see a separation in the place (+nasals) in the consonants. Silences get their own cluster.</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_4_biclusters.png" /></p>

<h3 id="biclusters-2">6 biclusters</h3>

<p>Fricatives and nasals get their own clusters.</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_6_biclusters.png" /></p>

<h2 id="phonemic-transcription">Phonemic transcription</h2>

<h3 id="biclusters-3">2 biclusters</h3>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phonemes_2_biclusters.png" /></p>

<h3 id="biclusters-4">4 biclusters</h3>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phonemes_4_biclusters.png" /></p>

<h3 id="biclusters-5">6 biclusters</h3>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phonemes_6_biclusters.png" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[phn2vec embeddings]]></title>
    <link href="http://snippyhollow.github.com/blog/2014/05/27/phn2vec-embeddings/"/>
    <updated>2014-05-27T21:00:00+02:00</updated>
    <id>http://snippyhollow.github.com/blog/2014/05/27/phn2vec-embeddings</id>
    <content type="html"><![CDATA[<p>Several months ago, I started thinking in terms of embeddings for everything,
let’s forget about discrete/categorical values and replace everything with
vector spaces that behave as we ask of them!<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<h3 id="xyz2vec">xyz2vec</h3>

<p>A few months ago, I toyed with <a href="http://radimrehurek.com/gensim/models/word2vec.html">“word2vec”</a> (<a href="http://arxiv.org/pdf/1301.3781.pdf">Mikolov et al. 2013</a>) in gensim on a lot of stuff. One of them were phonetic annotations of speech corpora. Basically, a word2vec model is a one hidden layer neural network trained with backpropagation of a loss based on a) either predicting the central word given its neighbors (continuous bag-of-word), or b) predicting the neighbors given the central word (skip-gram). This can be applied to corpora of continuous text of words, but anything that has neightboring structure really. So I ran word2vec on phonetic and phonemic datasets (TIMIT and Buckeye), with a window of 5 phone(me)s (+/- 2 around the central one) and both skip-grams (SG) and continuous bag-of-words (CBOW). For all the following results, I used an embedding dimension of 10, so it is “contractive” compared to the number of phone(me)s (39). I tried with 100 dimensions and this gave very similar results, so this does not seem to matter. All the code to reproduce these results is <a href="https://github.com/SnippyHolloW/speech_embeddings">here</a>.</p>

<h2 id="phone2vec">phone2vec</h2>

<p>Using <a href="https://catalog.ldc.upenn.edu/LDC93S1">TIMIT</a> phonetic annotations here are the similarity matrices of phones (SG left and CBOW right):</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_similarity_sg_left_cbow_right.png" /></p>

<p>If we do a 2 dimensional <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html">isomap projection</a> of the skip-grams, we can see 3 clusters of vowels, (mainly) plosives (stop consonants) and other consonants (some fricatives, nasals..).<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_isomap_sg.png" /></p>

<p>An isomap of the CBOW gives roughly the same clusters:</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_isomap_cbow.png" /></p>

<p>Contrary to the TIMIT corpus (read sentences that were designed for phonetic variability and effects), the <a href="http://buckeyecorpus.osu.edu/">Buckeye</a> is a corpus of conversational speech. We find the same (but a little bit weaker) clusters, e.g. in an isomap of skip-grams:</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/buckeye_phones_isomap_sg.png" /></p>

<h2 id="read-speech-vs-conversational-speech">read speech vs. conversational speech</h2>

<p>To the extent that the Buckeye and TIMIT corpus have slightly different phonetic annotations (and different annotations quality too), we can try and compare read speech vs. conversational speech. Here we plot the difference of the similarity matrices between one and the other (SG left, CBOW right). The biggest difference is in silences vs. stop-consonants:</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_vs_buckeye_phones_similarity_sg_left_cbow_right.png" /></p>

<h2 id="phoneme2vec">phoneme2vec</h2>

<p>What about phonemic annotation (phonemes from the word-level transcription)? Here are the similarity matrices (SG left, CBOW right):</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_words_similarity_sg_left_cbow_right.png" /></p>

<p>We still have the clusters of consonants vs vowels in the isomap of the skip-gram:</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_words_isomap_sg.png" /></p>

<p>and of the CBOW</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_words_isomap_cbow.png" /></p>

<p>but we have much lesser distinctions between front and back consonants as well as nasals and stop. That’s obvious, because phonotactics are not accounted for in the phonemic transcription that I did.</p>

<h2 id="phonetic-vs-phonemic">phonetic vs. phonemic</h2>

<p>We already know that speech (phonetics) and this “higher level” (phonemic) representation differ, how do they differ in this embedding?</p>

<p><img src="https://dl.dropboxusercontent.com/u/14035465/pictures/figures_10dim_5window/timit_phones_vs_words%28phonemes%29_similarity_sg_left_cbow_right.png" /></p>

<p>That’s all folks! Currently, I worked only on English datasets. That would be fun to see what comes up for other languages.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>There are several interesting papers about turning text into emdeddings that have useful properties. Picking a few: from classical nature language processing tasks done only with vector spaces (and neural networks) (<a href="http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/35671.pdf">Collobert et al. 2011</a>), to semantic spaces for multi-relational data (<a href="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf">Bordes et al. 2013</a>). The recent “word2vec” (<a href="http://arxiv.org/pdf/1301.3781.pdf">Mikolov et al. 2013</a>) from Google spiked interest from the NLP community, and it quickly got implemented in <a href="http://radimrehurek.com/gensim/">gensim</a> (if you want to geek out the implementation, I recommend the <a href="http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/">excellent blog post about its optimisation</a>).<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="https://en.wikipedia.org/wiki/Consonant#Features">Wikipedia provides some phonetics 101 of consonants</a>.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Collapsed Gibbs Sampling for Dirichlet Process Gaussian Mixture Models]]></title>
    <link href="http://snippyhollow.github.com/blog/2013/03/10/collapsed-gibbs-sampling-for-dirichlet-process-gaussian-mixture-models/"/>
    <updated>2013-03-10T15:44:00+01:00</updated>
    <id>http://snippyhollow.github.com/blog/2013/03/10/collapsed-gibbs-sampling-for-dirichlet-process-gaussian-mixture-models</id>
    <content type="html"><![CDATA[<p>I really enjoyed the pedagogy of <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Edwin Chen’s introduction to infinite mixture models</a>, but I was a little disappointed that it does not go as far as presenting the details of the Dirichlet process Gaussian mixture model (DPGMM), as he uses <a href="http://scikit-learn.org/stable/modules/mixture.html#dpgmm-classifier-infinite-gaussian-mixtures">sklearn’s variational Bayes DPGMM implementation</a>. </p>

<p>For this reason, I will try and give here sufficient information to implement a DPGMM with collapsed Gibbs sampling. This is not an <a href="http://www.is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/GoeRas10_[0].pdf">in-depth evaluation of which conjugate priors to use</a>, nor an analysis of the <a href="http://www.stat.duke.edu/courses/Spring06/sta376/Support/Mixtures/Escobar.West.1995.pdf">parameters</a> and <a href="ftp://dce.hut.edu.vn/vinhlt/Papers/GMM/Infinite%20GMM.pdf">hyper-parameters</a> (that should have their own priors! ;)).</p>

<h3 id="prerequisites">Prerequisites</h3>
<p>On Dirichlet processes, Chinese Restaurant processes, Indian Buffet processes, there is <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">the excellent blog post by Edwin Chen</a>. Another excellent introduction to Dirichlet processes is provided by <a href="http://www.ee.washington.edu/research/guptalab/publications/UWEETR-2010-0006.pdf">Frigyik, Kapila and Gupta</a>.</p>

<p>If you lack some knowledge about clustering or density estimation (unsupervised learning), you can read Chapters 20 (p. 284) to (at least) 22 of <a href="http://www.amazon.com/gp/product/0521642981/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0521642981&amp;linkCode=as2&amp;tag=syhwsblog-20">MacKay’s ITILA</a>, that you can find as a <a href="http://www.inference.phy.cam.ac.uk/itila/book.html">free ebook</a>; or chapter 9 of <a href="http://www.amazon.com/gp/product/0387310738/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0387310738&amp;linkCode=as2&amp;tag=syhwsblog-20">Bishop’s PRML</a>. As a refresher, the <a href="https://en.wikipedia.org/wiki/Mixture_model">Wikipedia article on mixture models</a>, and <a href="http://scikit-learn.org/stable/modules/mixture.html">the sklearn documentation on GMM</a> are more efficient.</p>

<h3 id="dpgmm-the-model">DPGMM: the model</h3>

<p>Let’s say we have $N$ observations and $K$ clusters, $i \in [1\dots N]$ is the indice for the observations, while $k \in [1\dots K]$ is the indice for the clusters. With $z_i$ the cluster assignment of observation $x_i$, and $\theta_k$ the parameter of mixture $k$:</p>

<script type="math/tex; mode=display">
P(x_{1:N}) = \prod_{i=1}^N \sum_{k=1}^K P(x_i|\theta_{z_i}) P(z_i=k)
</script>

<p>So, the generative story of a DPGMM is as follows:</p>

<p>$\pi \sim Stick(\alpha)$ (mixing rates)<br />
$z_i \sim \pi$ (cluster assignments)<br />
$\theta_k \sim H(\lambda)$ (parameters)<br />
$x_i \sim F(\theta_{z_i})$ (values)   </p>

<h3 id="fitting-the-data">Fitting the data</h3>
<p>Notation: </p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& z_{-i} = \{z_j | j \neq i\}\\
& x = x_{1:N}\\
& k^*\mathrm{\ is\ a\ new\ cluster}\\
& x_{-i,c} = \{x_j | z_j = c, j \neq i\}\\
& N_{k,-i} = card(x_{-i,c})
\end{align}
 %]]&gt;</script>

<p>Let’s decompose the probability that the observation $i$ belongs to cluster $k$ into its two independent factors:</p>

<script type="math/tex; mode=display">
P(z_i = k | z_{-i}, x, \alpha, \lambda) \propto P(z_i = k | z_{-i}, \alpha)P(x_i | x_{-i}, z_i=k, z_{-i}, \lambda)
</script>

<p>Then:</p>

<script type="math/tex; mode=display">
\boxed{P(z_i = k | z_{-i}, \alpha) = \left\{ 
\begin{array}{l}
\frac{N_{k,-i}}{\alpha + N - 1} \mathrm{\ if\ }k\ \mathrm{has\ been\ seen\ before}\\
\frac{\alpha}{\alpha + N - 1} \mathrm{\ if\ }k\ \mathrm{is\ a\ new\ cluster}\\
\end{array}
    \right.}
</script>

<script type="math/tex; mode=display">
P(x_i | x_{-i}, z_i=k, z_{-i}, \lambda) = P(x_i | x_{-i,k}, \lambda) = \frac{P(x_i, x_{-i, k}, \lambda)}{P(x_{-i,k}|\lambda)}
</script>

<script type="math/tex; mode=display">
\boxed{P(x_i, x_{-i, k}, \lambda) = \int P(x_i | \theta_k)\left[\prod_{j \neq i, z_j = k}P(x_j | \theta_k)\right] H(\theta_k | \lambda) d\theta_k}
</script>

<p>is the marginal likelihood of all the data assigned to cluster $k$, including $i$.</p>

<p>If $z_i = k^*$ (new cluster) then:</p>

<script type="math/tex; mode=display">
P(x_i | x_{-i}, z_i=k^*, z_{-i}, \lambda) = \boxed{P(x_i | \lambda) = \int P(x_i | \theta)H(\theta | \lambda) d\theta}
</script>

<h3 id="conjugate-priors">Conjugate priors</h3>

<p>Now we should choose $H$ for it to be conjugate to $F$ and have easy to compute parameters posterior. As we want $F$ to be multivariate normal: we can look on <a href="http://en.wikipedia.org/wiki/Conjugate_prior">Wikipedia’s page of conjugate priors</a> under multivariate normal with unknown $\mu$ and $\Sigma$ to see that $H$ should be normal-inverse-Wishart with prior parameters:</p>

<ul>
  <li>$\mu_0$ initial mean guess [In my code further, I set it to the mean of whole the dataset.]</li>
  <li>$\kappa_0$ mean fraction (smoothing parameter) [A common value is 1. I set it to 0.]</li>
  <li>$\nu_0$ degrees of freedom [I set it to the number of dimensions.]</li>
  <li>$\Psi_0$ pairwise deviation product (matrix) [I set it to $10 \times I_d$ ($I_d$ is the $d\times d$ identity matrix). Indentity matrix makes this prior Gaussian circular, the $10$ factor should be dependant on the dataset, for instance on the mean distance between points.]</li>
</ul>

<p>This gives us MAP estimates on parameters, for <em>one</em> of the clusters:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \mu_n = \frac{\kappa_0 \mu_0 + n\tilde{x}}{\kappa_0 +n} = \mu\\
& \kappa_n = \kappa_0 + n\\
& \nu_n = \nu_0 + n \\
& \Psi_n = \Psi_0 + C + \frac{\kappa_0 n}{\kappa_0 + n}(\tilde{x} - \mu_0)(\tilde{x} - \mu_0)^T\\
& \Sigma = \frac{\kappa_n + 1}{\kappa_n * (\nu_n - d + 1)}\Psi_n
\end{align}
 %]]&gt;</script>

<p>with $\tilde{x}$ the sample mean and $C=\sum_{i=1}^n (x_i-\tilde{x})(x_i-\tilde{x})^T$.</p>

<p>Set $\kappa_{0} = 0$ to have no effect of the prior on the posterior mean. 
This reduces to MLE estimates if:</p>

<script type="math/tex; mode=display">
\kappa_{0} = 0, \nu_{0} = d, \|\Psi_{0}\| = 0
</script>

<p>So now we can compute the posterior predictive for cluster $k$ evaluated at $x_i$</p>

<script type="math/tex; mode=display">
P(x_i | x_{-i}, z_i=k, z_{-i}, \lambda) \propto \mathcal{N}(\mu_{k,-i}, \Sigma_{k,-i})
</script>

<h3 id="collapsed-gibbs-sampling">Collapsed Gibbs sampling</h3>

<p>Here is the pseudo-code of collapsed Gibbs sampling adapted from algorithm 3 of <a href="http://www.stat.purdue.edu/~rdutta/24.PDF">Neal’s seminal paper</a>:</p>

<pre><code>while (not converged on mus and sigmas):
    for each i = 1 : N in random order do:
        remove x[i]'s sufficient statistics from old cluster z[i]
        if any cluster is empty, remove it and decrease K
        for each k = 1 : K do
            compute P_k(x[i]) = P(x[i] | x[-i]=k)
            N[k,-i] = dim(x[-i]=k)
            compute P(z[i]=k | z[-i], Data) = N[k,-i] / (alpha + N - 1)
        compute P*(x[i]) = P(x[i] | lambda)
        compute P(z[i]=* | z[-i], Data) = alpha / (alpha + N - 1)
        normalize P(z[i] | ...)
        sample z[i] from P(z[i] | ...)
        add x[i]'s sufficient statistics to new cluster z[i]
        (possibly increase K)
</code></pre>

<h3 id="results">Results</h3>

<p>Here is the result of our implementation of collapsed Gibbs sampling DPGMM compared to scikit-learn’s implementation of <a href="http://scikit-learn.org/stable/modules/mixture.html#dpgmm-classifier-infinite-gaussian-mixtures">variational Bayes DPGMM</a>:</p>

<p><img src="https://dl.dropbox.com/u/14035465/pictures/DPGMM.png" /></p>

<h3 id="code">Code</h3>

<p>Here is my quick-and-dirty code implementing this version of Gibbs sampling for DPGMM. You may want to comment out scikit-learn (that I used for the comparison above) if you do not have it installed.</p>

<div><script src="https://gist.github.com/5128969.js"></script>
<noscript><pre><code /></pre></noscript></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[From hacks to Bayesian probability]]></title>
    <link href="http://snippyhollow.github.com/blog/2013/03/08/from-hacks-to-bayesian-probability/"/>
    <updated>2013-03-08T18:58:00+01:00</updated>
    <id>http://snippyhollow.github.com/blog/2013/03/08/from-hacks-to-bayesian-probability</id>
    <content type="html"><![CDATA[<p>In which we look at two pragmatic hacks that lead to the Bayesian approach of probabilities, when pushed further and added as constraints.</p>

<h2 id="coinflips">Coinflips</h2>

<p>Let’s say we have a coin, and we want to decide if it’s fair. We throw it $N$ times and we get $m$ heads, we can code heads=1, tails=0. With $\mu$ the ratio of heads:</p>

<script type="math/tex; mode=display"> 
P(m | N, \mu) = Binomial(m|N,\mu) = \binom{N}{m} \mu^m (1-\mu)^{N-m}
</script>

<h3 id="maximum-likelihood">Maximum likelihood</h3>
<p>How do we set $\mu$? We could maximize the probability of the data that we saw under our model, that is maximizing the <em>likelihood</em>. Let’s say that $D = {x_1 \dots x_N}$, then we have:</p>

<script type="math/tex; mode=display">
P(D|\mu) = \prod_{n=1}^N P(x_n|\mu) = \prod_{n=1}^N \mu^{x_n}(1-\mu)^{1-x_n}
</script>

<p>The maximum of this function of $\mu$ is reached for $\mu= \frac{m}{N}$. The problem arises if we have little data (in fact, when we have data that does not cover the whole space of possible data). If $D=(1,1,1)$, the maximum likelihood estimate of $\mu$ will be $1.0$. It means that we predict that <em>all</em> the tosses will land on heads, after only three observations!</p>

<h3 id="smoothing">Smoothing</h3>

<p>A classical hack is to smooth the maximum likelihood estimate by adding “fake data”, we could consider that we already saw the coin land on heads and tails once, before getting our data. This way, before (“prior to”) the experiment, we would have $\mu=1/2=0.5$. After (<em>posterior</em> to) our experiment, taking the data into account, we would have $\mu = (3+1)/(3+2) = 0.8$. How do we set the these prior coin flips (smoothing parameters)?</p>

<h3 id="maximum-a-posteriori">Maximum A Posteriori</h3>

<p>The <em>right way</em> to encode this prior knowledge is to put a probability distribution on the parameter $\mu$. As $\mu$ is a ratio, we should have a continuous distribution on $[0, 1]$ that can represent a whole range of prior belief on what the coin’s ratio of heads is. For these reasons, a sensible choice is the Beta distribution:</p>

<script type="math/tex; mode=display">
Beta(\mu|a, b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}
</script>

<p>On Wikpedia, we can check how the $Beta(x|\alpha, \beta)$ distribution looks like:</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Beta_distribution_pdf.svg/639px-Beta_distribution_pdf.svg.png" alt="Plots of the Beta distribution" /></p>

<p>Now we can compute again what is the <em>posterior</em> value of $\mu$ knowing the data $D$ and the prior Beta ($\propto$ means “proportional to”):</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
P(\mu | a_0, b_0, D) & \propto P(D|\mu)P(\mu|a_0, b_0)\\
                    & \propto \left( \prod_{n=1}^N \mu^{x_n} (1-\mu)^{1-x_n} \right) Beta(\mu | a_0, b_0)
\end{align}
 %]]&gt;</script>

<p>Hopefully, the Beta distribution is the conjugate prior for the Bernouilli and binomial distributions, and thus a bit of calculus reduces it to:</p>

<script type="math/tex; mode=display">
P(\mu | a_0, b_0, D) \propto Beta(\mu|a_N, b_N)\\
a_N = a_0 + m\\
b_N = b_0 + (N-m)
</script>

<p>We can compute that, when $N \rightarrow \infty$, the expectation of $\mu$: $\mathbb{E}[\mu] = \mu_{ML}$, as:</p>

<script type="math/tex; mode=display">
\mathbb{E} [\mu | a_0, b_0, D] = \frac{a_N}{b_N}
</script>

<h3 id="first-conclusion">First conclusion</h3>

<p>This approach of using a prior on the parameters of the distributions that are essential to our model (the predicting distribution) is central to the Bayesian approach of building models. It makes the model robust to what can happen, even though we had few data. It makes it easier to reason about our prior assumptions that simply “adding unseen data”, and it yields in the presence of more data.</p>

<p>If you’re interested about Bayesian modeling, there are plenty of very good textbooks. My prefered gradual introduction is <a href="http://www.amazon.com/gp/product/0521642981/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0521642981&amp;linkCode=as2&amp;tag=syhwsblog-20">MacKay’s ITILA</a>, that you can find as a <a href="http://www.inference.phy.cam.ac.uk/itila/book.html">free ebook</a>.</p>

<h2 id="causality">Causality</h2>

<p>Now here is another hack for logical reasoning, that leads to Bayesian probabilities. Let’s say that you want to express that an event $A$ entails an event $B$, in logic you would write $A \Rightarrow B$. We will be abusing the notation $A=[A=true]$ and $\neg A=[A=false]$. Now with the <a href="http://en.wikipedia.org/wiki/Modus_ponens"><em>modus ponens</em></a>, you can deduce $B$ whenever $A$ is true.</p>

<script type="math/tex; mode=display">
\frac{[A\Rightarrow B] \wedge A}{B}
</script>

<h3 id="plausible-reasoning">Plausible reasoning</h3>

<p>Now, we want to extend prepositional logic to <em>plausible reasoning</em>, in which we can have degrees of probability that rules are true; or degrees of belief in these rules and facts. A pragmatic way to do that is to introduce the variable $C$ which represents $A \Rightarrow B$, that is: if $P(C)=p$, there is a probability $p$ that $A \Rightarrow B$. Then, this previous <em>modus ponens</em> translates to:</p>

<script type="math/tex; mode=display">
P(B|A,C) = \frac{P(A|B,C)P(B|C)}{P(A|C)}\ (Bayes'\ theorem)\\
P(B|A,C)=\frac{P(A,B|C)}{P(A|C)}\ (Product\ rule)
</script>

<p>And actually, as $P(A,B|C)=P(A|C)$, we have $P(B|A,C)=1$, which corresponds to the strong syllogism of <em>modus ponens</em>. </p>

<p>So now, if we are only 80% sure of $C$, we can write $P(C) = 0.8$ and seek for $P(B|A)$ (we are 100% sure of A):</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
P(B|A) = \frac{\sum_{C\in\{false,true\}} P(B|A,C)P(A)P(C)}{P(A)} & = P(\neg C)P(B|A,\neg C) + P(C)P(B|A,C)\\
& = 0.2*x(\in [0,1]) + 0.8*1.0 \geq 0.8
\end{align}
 %]]&gt;</script>

<p>Which means that $B$ has 80% chances to be true by following the strong syllogism of modus ponens, but it can also be true even though $C=false$.</p>

<p>Finally, contrary to prepositional logic, we <em>also</em> get the weak syllogism (and I’ll let you think it through):</p>

<script type="math/tex; mode=display">
\frac{[A\Rightarrow B] \wedge B}{A\ becomes\ more\ plausible}
</script>

<p>A similar derivation and observation can be done for <a href="http://en.wikipedia.org/wiki/Modus_tollens"><em>modus tollens</em></a>.</p>

<h3 id="cox-jaynes-theorem">Cox-Jaynes theorem</h3>

<p>A reasoning mechanism needs to be consistent (one cannot prove $A$ and $\neg A$ at the same time). For plausible reasoning, consistency means: a) all the possible ways to reach a conclusion leads to the same result, b) information cannot be ignored, c) two equal states of knowledge have the same plausibilities. Adding consistency to plausible reasoning leads to <a href="http://en.wikipedia.org/wiki/Cox's_theorem">Cox’s theorem</a>, which derives the laws of probability (the product-rule and the sum-rule). So, the degrees of belief of any consistent induction mechanism verify Kolmogorov’s axioms.</p>

<h3 id="second-and-last-conclusion">Second and last conclusion</h3>

<p>With plausible reasoning, we get all the benefits of prepositional logic, but we can also reason with/about facts and rules that are not 100% true. We have another example of how a pragmatical (sensical) hack to extend logic to “degrees of beliefs” (probabilities) leads to Bayesian probabilities. </p>

<p>If you are interested by learning about plausible reasonning, you can <a href="http://emotion.inrialpes.fr/people/synnaeve/phdthesis/phdthesis.html#x1-590003.2">look at my thesis</a>, or, better yet, read it directly from one of the masters in <a href="http://www.amazon.com/gp/product/0521592712/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0521592712&amp;linkCode=as2&amp;tag=syhwsblog-20">Jayne’s Probability Theory: The Logic of Science</a> for which the pre-print is <a href="http://www-biba.inrialpes.fr/Jaynes/prob.html">there</a>.</p>

]]></content>
  </entry>
  
</feed>
