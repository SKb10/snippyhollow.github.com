---
layout: post
title: "You could have come up with EM"
date: 2013-03-13 16:05
comments: true
published: false
categories: [Bayesian, EM, mixture models]
---

The problem that the expectation-maximization (EM) algorithm solves is simple to state. Given observed variables $X$ and hidden variables $Z$, the goal is to find the set of parameters $\theta$ that maximizes the likelihood:

$$
P(X|\theta) = \sum_{Z} P(X, Z|\theta)
$$

### EM as averaging

Consider that the complete data is $D = X \cup Z$. If you have that, it is very easy to find the parameters $\theta$ that maximizes $P(D\| \theta)$. So we just have to first complete $D$ with our best _expectation_ of $Z$ (according to what already know about our model) and then we can find the parameters $\theta$ that _maximize_ $P(D \| \theta)$. Hopefully, doing that several times will converge on a given set of parameters $\theta$ (it does! it's proven later in this post).

So lets take some data:

$$
\begin{array}{|l|cccccc|}
\hline
\mathrm{indice\ of\ observation} & 0 & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
\mathrm{value} & 0 & ? & 10 & ? & 11 & -1 & 0\\
\hline
\end{array}
$$

If we assume that the observations are generated by two Gaussian distributions (the "2" here is a parameter, it's encoded in the cardinal of the mixing rate $\pi$ in the parameters $\theta$), we can start with:

$$
\theta = (\pi, \mu_0, \sigma_0, \mu_1, \sigma_1)
$$

in which 

$$
\begin{align}
& \mu_k \sim \mathcal{N}(\mu_{ML(X)}, \sigma_{ML(X)})\\
& \sigma_k = \sigma_{ML(X)|\mu_{k}}\\
\mathrm{so} & \mu_{ML(X)} = 4, \sigma_{ML(X)} \approx 5.33\\
\mathrm{we\ can\ sample} & \mu_0 = 3.7, \sigma_0 = 5.337\\
& \mu_1 = 4.4, \sigma_1 = 5.344
\end{align}
$$

For maximum-likelihood (ML), the $sigma$ are Easily computed for all $\mu$ with:

    X = [0, 10, 11, -1, 0]
    sigma = math.sqrt(sum(map(lambda x: (x-mu)*(x-mu), X))*1.0/len(X))

TODO

$$
\begin{array}{|l|cccccc|}
\hline
\mathrm{indice\ of\ observation} & 0 & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
\mathrm{feature\ 1\ value} & 0 & 1 & 10 & 11 & 9 & ? & -1\\
\mathrm{feature\ 2\ value} & 0 & ? & 10 & ? & 11 & -1 & 0\\
\hline
\end{array}
$$

TODO

"But, how come we use EM even when we have full observations?" By following exactly the same reasoning but considering that the cluster assignments of each observation is a hidden (latent) value. This way we _expect_ these values (maximize their probability) and then pick parameters that _maximize_ the (new) likelihood.

http://www.svcl.ucsd.edu/courses/ece271A-F08/handouts/EM2.pdf



### EM as (soft) K-means

TODO (MacKay / Bishop)

K-means cost function: 

$$
J = \sum_{n=1}^{N} \sum_{k=1}^{K} resp_{n,k}||x_n - \mu_k||^2
$$

{% img https://dl.dropbox.com/u/14035465/pictures/kmeans_EM.png %}

### EM as calculatory tricks

Let us come back to:

$$
P(X|\theta) = \sum_{Z} P(X, Z|\theta)
$$

We can also write (product rule):

$$
P(X, Z|\theta) = P(Z|X, \theta)P(X|\theta)\\
P(X|\theta) = \frac{P(Z|X, \theta)}{P(X, Z|\theta)}
$$

and take the log:

$$
\ln P(X, Z|\theta) = \ln P(Z|X, \theta) + \ln P(X|\theta)\\
\ln P(X|\theta) = \ln P(Z|X, \theta) - \ln P(X, Z|\theta)
$$

We can also multiply and divide by the same quantity:

$$
P(X|\theta) = \frac{Q(Z)P(Z|X, \theta)}{Q(Z)P(X, Z|\theta)}
$$


