---
published: false
layout: post
title: "You could have come up with EM"
date: 2013-03-13 16:05
comments: true
categories: [Bayesian, EM, mixture models]
---

The problem that the EM algorithm solves is simple to state. Given observed variables $X$ and hidden variables $Z$, the goal is to find the set of parameters $\theta$ that maximizes the likelihood:

$$
P(X|\theta) = \sum_{Z} P(X, Z|\theta)
$$

### EM as averaging

Consider that the complete data is $D = X \cup Z$. If you have that, it is very easy to find the parameters $\theta$ that maximizes $P(D\| \theta)$. So we just have to first complete $D$ with our best _expectation_ of $Z$ (according to what already know about our model) and then we can find the parameters $\theta$ that _maximize_ $P(D \| \theta)$. Hopefully, doing that several times will converge on a given set of parameters $\theta$ (it does! it's proven later in this post).

So lets take some data:

$$
\begin{array}{|l|cccccc|}
\hline
\mathrm{indice\ of\ observation} & 0 & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
\mathrm{value} & 0 & ? & 10 & ? & 11 & -1 & 0\\
\hline
\end{array}
$$

If we assume that the observations are generated by two Gaussian distributions (the "2" here is a parameter, it's encoded in $\theta$), we can start with:

$$
\theta = (\mu_0, \sigma_0, \mu_1, \sigma_1)
$$

in which 

$$
\mu_k \sim \mathcal{N}(\mu_{ML(X)}, \sigma_{ML(X)})\\
\sigma_k = \sigma_{ML(X)|\mu_{k}}
$$

TODO

$$
\begin{array}{|l|cccccc|}
\hline
\mathrm{indice\ of\ observation} & 0 & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
\mathrm{feature\ 1\ value} & 0 & 1 & 10 & 11 & 9 & ? & -1\\
\mathrm{feature\ 2\ value} & 0 & ? & 10 & ? & 11 & -1 & 0\\
\hline
\end{array}
$$

TODO


### EM as K-means

TODO (MacKay / Bishop)

### EM as calculatory tricks

Let us come back to:

$$
P(X|\theta) = \sum_{Z} P(X, Z|\theta)
$$

We can also write (product rule):

$$
P(X, Z|\theta) = P(Z|X, \theta)P(X|\theta)
$$

and take the log:

$$
\ln P(X, Z|\theta) = \ln P(Z|X, \theta) + \ln P(X|\theta)
$$

We can also multiply and 


